## 第一步：为controller-manager 提供 rbd 命令

使用StorageClass动态创建PV时，controller-manager会自动在Ceph上创建image，所以我们要为其准备好rbd命令。

1. 如果k8s集群是用 kubeadm 部署的，由于 controller-manager 官方镜像中没有 `rbd` 命令，所以我们要提供 `rbd`命令来驱动 ceph。
    
    ```yaml
    kind: ClusterRole 
    apiVersion: rbac.authorization.k8s.io/v1 
    metadata: 
      name: rbd-provisioner 
    rules: 
      - apiGroups: [""] 
        resources: ["persistentvolumes"] 
        verbs: ["get", "list", "watch", "create", "delete"] 
      - apiGroups: [""] 
        resources: ["persistentvolumeclaims"] 
        verbs: ["get", "list", "watch", "update"] 
      - apiGroups: ["storage.k8s.io"] 
        resources: ["storageclasses"] 
        verbs: ["get", "list", "watch"] 
      - apiGroups: [""] 
        resources: ["events"] 
        verbs: ["create", "update", "patch"] 
      - apiGroups: [""] 
        resources: ["services"] 
        resourceNames: ["kube-dns","coredns"] 
        verbs: ["list", "get"] 
    --- 
    kind: ClusterRoleBinding 
    apiVersion: rbac.authorization.k8s.io/v1 
    metadata: 
      name: rbd-provisioner 
    subjects: 
      - kind: ServiceAccount 
        name: rbd-provisioner 
        namespace: default 
    roleRef: 
      kind: ClusterRole 
      name: rbd-provisioner 
      apiGroup: rbac.authorization.k8s.io 
    --- 
    apiVersion: rbac.authorization.k8s.io/v1 
    kind: Role 
    metadata: 
      name: rbd-provisioner 
    rules: 
    - apiGroups: [""] 
      resources: ["secrets"] 
      verbs: ["get"] 
    - apiGroups: [""] 
      resources: ["endpoints"] 
      verbs: ["get", "list", "watch", "create", "update", "patch"] 
    --- 
    apiVersion: rbac.authorization.k8s.io/v1 
    kind: RoleBinding 
    metadata: 
      name: rbd-provisioner 
    roleRef: 
      apiGroup: rbac.authorization.k8s.io 
      kind: Role 
      name: rbd-provisioner 
    subjects: 
      - kind: ServiceAccount 
        name: rbd-provisioner 
        namespace: default 
    --- 
    apiVersion: extensions/v1beta1 
    kind: Deployment 
    metadata: 
      name: rbd-provisioner
    spec: 
      replicas: 1 
      strategy: 
        type: Recreate 
      template: 
        metadata: 
          labels: 
            app: rbd-provisioner 
        spec: 
          containers: 
          - name: rbd-provisioner 
            image: quay.io/external_storage/rbd-provisioner:latest 
            env: 
            - name: PROVISIONER_NAME 
              value: ceph.com/rbd 
          serviceAccount: rbd-provisioner 
    --- 
    apiVersion: v1 
    kind: ServiceAccount 
    metadata: 
      name: rbd-provisioner
    ```
    
     **注意**：rbd-provisioner 的镜像要和 ceph 的版本适配，这里镜像使用最新的，根据官方提示已支持ceph mimic版。
    
     ![](/img/provisioner.png)

2. 如果 k8s 集群使用二进制方式部署的，直接在 master 节点上安装 ceph-common 即可
    
    * 配置 yum 源
    
    * 安装客户端：`yum -y install ceph-common`
    
    * 拷贝 keyring 文件
      
      将 ceph 的 ceph.client.admin.keyring 文件拷贝到 master 的 `/etc/ceph` 目录下。

## 第二步：为 kubelet 提供 rbd 命令

创建 pod 时，kubelet 需要使用 `rbd` 命令去检测和挂载 pv 对应的 ceph image，所以要在**所有的 worker 节**点安装 ceph 客户端 ceph-common-13.2.5。
